# Servi√ßo LLM - SongMetrix

Este documento descreve o servi√ßo LLM implementado para gera√ß√£o autom√°tica de conte√∫do de e-mails de insights no SongMetrix.

## Vis√£o Geral

O `LlmService` √© um adaptador TypeScript que permite comunica√ß√£o com diferentes APIs de LLM (Large Language Models) para gerar conte√∫do personalizado de e-mails baseado em dados de insights musicais.

## Arquitetura

### Componentes Principais

1. **`LlmService`** - Classe principal do servi√ßo
2. **Tabela `llm_provider_settings`** - Configura√ß√µes dos provedores LLM
3. **Scripts de configura√ß√£o** - Ferramentas para setup e teste
4. **Logger Winston** - Sistema de logs detalhado

### Fluxo de Funcionamento

```
1. generateEmailContent() chamado com insightData
2. Busca provedor ativo na tabela llm_provider_settings
3. Switch baseado no provider_name
4. Chama m√©todo espec√≠fico do provedor (_callOpenAiApi)
5. Retorna { subject, body_html }
```

## Estrutura de Arquivos

```
src/services/llmService.ts          # Classe principal do servi√ßo
supabase/migrations/
  create_llm_provider_settings_table.sql  # Migra√ß√£o da tabela
scripts/
  setup-llm-provider.js             # Script de configura√ß√£o
  test-llm-service.js               # Script de teste
docs/llm-service.md                 # Esta documenta√ß√£o
```

## Configura√ß√£o Inicial

### 1. Executar Migra√ß√£o

Execute o SQL no Supabase para criar a tabela:

```sql
-- Execute o conte√∫do de:
-- supabase/migrations/create_llm_provider_settings_table.sql
```

### 2. Configurar Provedor

Use o script interativo para configurar o primeiro provedor:

```bash
npm run setup-llm
```

O script ir√°:
- Verificar provedores existentes
- Solicitar API key da OpenAI
- Configurar modelo, tokens e temperatura
- Testar a configura√ß√£o

### 3. Testar Servi√ßo

Execute os testes para verificar funcionamento:

```bash
npm run test-llm
```

## API da Classe LlmService

### M√©todo P√∫blico

#### `generateEmailContent(insightData: any): Promise<EmailContent>`

Gera conte√∫do de e-mail baseado em dados de insight.

**Par√¢metros:**
- `insightData`: Objeto com dados do insight musical

**Retorno:**
```typescript
interface EmailContent {
  subject: string;      // Assunto do e-mail
  body_html: string;    // Corpo em HTML
}
```

**Exemplo de uso:**
```typescript
import { LlmService } from './services/llmService';

const llmService = new LlmService();

const insightData = {
  userId: 'user-123',
  insightType: 'growth_trend',
  metrics: {
    totalPlays: 250,
    topArtist: 'Anitta',
    topSong: 'Envolver',
    growthPercentage: 35,
    uniqueArtists: 35,
    uniqueSongs: 120,
    uniqueRadios: 8
  }
};

const emailContent = await llmService.generateEmailContent(insightData);
console.log('Assunto:', emailContent.subject);
console.log('HTML:', emailContent.body_html);
```

### M√©todos Privados

#### `_callOpenAiApi(apiKey: string, insightData: any, settings: LlmProviderSettings)`

M√©todo privado para comunica√ß√£o com a API da OpenAI.

**Caracter√≠sticas:**
- Usa `response_format: { type: 'json_object' }` para garantir JSON v√°lido
- Suporte a diferentes modelos (gpt-4o, gpt-3.5-turbo)
- Configura√ß√µes personaliz√°veis (max_tokens, temperature)
- Logs detalhados de uso e performance

## Tabela llm_provider_settings

### Estrutura

```sql
CREATE TABLE llm_provider_settings (
  id UUID PRIMARY KEY,
  provider_name TEXT UNIQUE,           -- 'OpenAI', 'Anthropic', etc.
  api_key TEXT NOT NULL,               -- Chave da API
  api_url TEXT,                        -- URL da API (opcional)
  model_name TEXT,                     -- Nome do modelo
  max_tokens INTEGER DEFAULT 1000,    -- M√°ximo de tokens
  temperature DECIMAL(3,2) DEFAULT 0.7, -- Temperatura (0-2)
  is_active BOOLEAN DEFAULT FALSE,     -- Apenas um pode estar ativo
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW(),
  created_by UUID REFERENCES auth.users(id)
);
```

### Pol√≠ticas de Seguran√ßa

- **RLS habilitado**: Apenas admins podem ver/modificar
- **Trigger de ativa√ß√£o**: Apenas um provedor pode estar ativo
- **Valida√ß√µes**: Temperature (0-2), max_tokens (1-8000)

### Provedores Suportados

| Provedor | Status | Modelos Suportados |
|----------|--------|-------------------|
| OpenAI | ‚úÖ Implementado | gpt-4o, gpt-3.5-turbo |
| Anthropic | üîÑ Planejado | Claude-3 |
| Google | üîÑ Planejado | Gemini |
| Azure | üîÑ Planejado | Azure OpenAI |

## Logs e Monitoramento

### N√≠veis de Log

- **INFO**: Opera√ß√µes normais, in√≠cio/fim de processos
- **DEBUG**: Detalhes de prompts e configura√ß√µes
- **ERROR**: Erros de API, configura√ß√£o ou parsing
- **WARN**: Situa√ß√µes que merecem aten√ß√£o

### Exemplos de Logs

```json
{
  "level": "info",
  "message": "[LlmService] Iniciando gera√ß√£o de conte√∫do de e-mail",
  "timestamp": "2024-01-15T10:30:00.000Z",
  "insightType": "growth_trend",
  "userId": "user-123"
}

{
  "level": "info", 
  "message": "[LlmService] Resposta recebida da OpenAI",
  "timestamp": "2024-01-15T10:30:05.000Z",
  "usage": {
    "prompt_tokens": 150,
    "completion_tokens": 200,
    "total_tokens": 350
  },
  "model": "gpt-4o",
  "finishReason": "stop"
}
```

## Tratamento de Erros

### Tipos de Erro

1. **Provedor n√£o encontrado**: Nenhum provedor ativo configurado
2. **Erro de API**: Problemas na comunica√ß√£o com o LLM
3. **Parsing JSON**: Resposta inv√°lida do LLM
4. **Valida√ß√£o**: Dados de entrada inv√°lidos

### Estrat√©gias de Recupera√ß√£o

- **Logs detalhados**: Todos os erros s√£o registrados com contexto
- **Propaga√ß√£o controlada**: Erros s√£o re-lan√ßados com mensagens claras
- **Fallback futuro**: Possibilidade de implementar fallback para templates est√°ticos

## Seguran√ßa

### Prote√ß√£o de API Keys

- **Armazenamento**: API keys ficam na tabela protegida por RLS
- **Acesso**: Apenas admins podem ver/modificar configura√ß√µes
- **Logs**: API keys nunca aparecem nos logs (apenas presen√ßa/aus√™ncia)

### Valida√ß√£o de Entrada

- **Sanitiza√ß√£o**: Dados de insight s√£o serializados como JSON
- **Limites**: max_tokens e temperature t√™m valida√ß√µes de range
- **Timeout**: Chamadas de API t√™m timeout impl√≠cito

## Performance

### Otimiza√ß√µes

- **Cache de configura√ß√£o**: Configura√ß√µes s√£o buscadas uma vez por chamada
- **Logs estruturados**: Permitem an√°lise de performance
- **M√©tricas de uso**: Tokens utilizados s√£o registrados

### Limites Recomendados

- **max_tokens**: 1000-2000 para e-mails (balance qualidade/custo)
- **temperature**: 0.7 para criatividade moderada
- **timeout**: 30 segundos para chamadas de API

## Exemplos de Uso

### Configura√ß√£o B√°sica

```bash
# 1. Executar migra√ß√£o no Supabase
# 2. Configurar provedor
npm run setup-llm

# 3. Testar configura√ß√£o
npm run test-llm
```

### Integra√ß√£o com Outros Servi√ßos

```typescript
// Em um servi√ßo de e-mail
import { LlmService } from './llmService';

class EmailService {
  private llmService = new LlmService();
  
  async sendInsightEmail(userId: string, insightData: any) {
    try {
      // Gerar conte√∫do com LLM
      const emailContent = await this.llmService.generateEmailContent(insightData);
      
      // Enviar e-mail
      await this.sendEmail({
        to: user.email,
        subject: emailContent.subject,
        html: emailContent.body_html
      });
      
    } catch (error) {
      console.error('Erro ao enviar insight:', error);
      // Fallback para template est√°tico
    }
  }
}
```

### Administra√ß√£o via API

```javascript
// Listar provedores (admin)
fetch('/api/admin/llm-settings', {
  headers: { 'Authorization': `Bearer ${token}` }
})

// Atualizar configura√ß√µes (admin)
fetch('/api/admin/llm-settings', {
  method: 'PUT',
  headers: { 
    'Authorization': `Bearer ${token}`,
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({
    llm_model: 'gpt-4o',
    max_tokens: 1500,
    temperature: 0.8
  })
})
```

## Troubleshooting

### Problemas Comuns

#### "Nenhum provedor LLM ativo encontrado"
- **Causa**: Tabela vazia ou nenhum provedor marcado como ativo
- **Solu√ß√£o**: Execute `npm run setup-llm` para configurar

#### "Erro na API da OpenAI: 401 Unauthorized"
- **Causa**: API key inv√°lida ou expirada
- **Solu√ß√£o**: Verifique a API key em https://platform.openai.com/api-keys

#### "Resposta da API n√£o cont√©m as chaves necess√°rias"
- **Causa**: LLM n√£o retornou JSON no formato esperado
- **Solu√ß√£o**: Ajuste o prompt ou temperature

#### "Timeout na chamada da API"
- **Causa**: API lenta ou indispon√≠vel
- **Solu√ß√£o**: Verifique status da API ou ajuste timeout

### Logs de Debug

Para ativar logs detalhados:

```bash
export LOG_LEVEL=debug
npm run test-llm
```

### Verifica√ß√£o de Sa√∫de

```sql
-- Verificar provedor ativo
SELECT provider_name, model_name, is_active, updated_at 
FROM llm_provider_settings 
WHERE is_active = true;

-- Verificar √∫ltimas configura√ß√µes
SELECT * FROM llm_provider_settings 
ORDER BY updated_at DESC 
LIMIT 5;
```

## Roadmap

### Pr√≥ximas Funcionalidades

1. **M√∫ltiplos Provedores**: Anthropic, Google, Azure
2. **Fallback Autom√°tico**: Trocar provedor em caso de falha
3. **Cache de Respostas**: Evitar chamadas desnecess√°rias
4. **Templates Personalizados**: Prompts espec√≠ficos por tipo de insight
5. **M√©tricas Avan√ßadas**: Dashboard de uso e performance
6. **Rate Limiting**: Controle de chamadas por usu√°rio/per√≠odo

### Melhorias Planejadas

1. **Valida√ß√£o de Prompt**: Verificar qualidade antes do envio
2. **A/B Testing**: Testar diferentes prompts/modelos
3. **Personaliza√ß√£o**: Prompts baseados no perfil do usu√°rio
4. **Multil√≠ngua**: Suporte a diferentes idiomas