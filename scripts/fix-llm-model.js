#!/usr/bin/env node

/**
 * Script para corrigir o modelo LLM invÃ¡lido
 */

import { createClient } from '@supabase/supabase-js';
import dotenv from 'dotenv';

// Carregar variÃ¡veis de ambiente
dotenv.config();

const supabaseUrl = process.env.VITE_SUPABASE_URL || process.env.SUPABASE_URL;
const supabaseServiceKey = process.env.SUPABASE_SERVICE_KEY || process.env.SUPABASE_SERVICE_ROLE_KEY;

if (!supabaseUrl || !supabaseServiceKey) {
  console.error('âŒ VariÃ¡veis de ambiente do Supabase nÃ£o configuradas');
  process.exit(1);
}

const supabase = createClient(supabaseUrl, supabaseServiceKey, {
  auth: {
    autoRefreshToken: false,
    persistSession: false
  }
});

async function fixLLMModel() {
  try {
    console.log('ðŸ”§ Corrigindo modelo LLM invÃ¡lido...\n');

    // 1. Encontrar o provedor ativo com modelo invÃ¡lido
    const { data: activeProvider, error: findError } = await supabase
      .from('llm_provider_settings')
      .select('*')
      .eq('is_active', true)
      .eq('model_name', 'gpt-5')
      .single();

    if (findError) {
      console.error('âŒ Erro ao buscar provedor ativo:', findError.message);
      return;
    }

    if (!activeProvider) {
      console.log('â„¹ï¸  Nenhum provedor com modelo gpt-5 encontrado');
      return;
    }

    console.log(`ðŸ“ Provedor encontrado: ${activeProvider.provider_name}`);
    console.log(`   Modelo atual: ${activeProvider.model_name} âŒ`);
    console.log(`   ID: ${activeProvider.id}`);

    // 2. Atualizar para um modelo vÃ¡lido
    const newModel = 'gpt-4o-mini'; // Modelo vÃ¡lido e eficiente
    
    console.log(`\nðŸ”„ Atualizando modelo para: ${newModel}`);

    const { data: updatedProvider, error: updateError } = await supabase
      .from('llm_provider_settings')
      .update({
        model_name: newModel,
        updated_at: new Date().toISOString()
      })
      .eq('id', activeProvider.id)
      .select()
      .single();

    if (updateError) {
      console.error('âŒ Erro ao atualizar provedor:', updateError.message);
      return;
    }

    console.log('âœ… Modelo atualizado com sucesso!');
    console.log(`   Novo modelo: ${updatedProvider.model_name}`);
    console.log(`   Provedor: ${updatedProvider.provider_name}`);
    console.log(`   Status: ${updatedProvider.is_active ? 'Ativo' : 'Inativo'}`);

    console.log('\nðŸŽ‰ CorreÃ§Ã£o concluÃ­da! Agora tente gerar o insight personalizado novamente.');

  } catch (error) {
    console.error('ðŸ’¥ Erro ao corrigir modelo LLM:', error.message);
  }
}

// Executar correÃ§Ã£o
fixLLMModel().then(() => {
  console.log('\nðŸ CorreÃ§Ã£o concluÃ­da!');
  process.exit(0);
}).catch(error => {
  console.error('ðŸ’¥ Erro fatal:', error);
  process.exit(1);
});